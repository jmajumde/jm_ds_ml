{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This excercise is to follow along with discussion https://www.kaggle.com/tboyle10/methods-for-dealing-with-imbalanced-data/notebook and https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods for Dealing with Imbalanced Data\n",
    "Imbalanced classes are a common problem in machine learning classification where there are a disproportionate ratio of observations in each class. Class imbalance can be found in many different areas including medical diagnosis, spam filtering, and fraud detection.\n",
    "\n",
    "In this guide, we'll look at five possible ways to handle an imbalanced class problem using credit card data. Our objective will be to correctly classify the minority class of fraudulent transactions.\n",
    "\n",
    "Important Note: This guide will focus soley on addressing imbalanced classes and will not addressing other important machine learning steps including, but not limited to, feature selection or hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up default plotting parameters\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20.0, 7.0]\n",
    "plt.rcParams.update({'font.size': 22,})\n",
    "\n",
    "sns.set_palette('viridis')\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk', font_scale=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.Class.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using seaborns countplot to show distribution of questions in dataset\n",
    "fig, ax = plt.subplots()\n",
    "g = sns.countplot(df.Class, palette='viridis')\n",
    "g.set_xticklabels(['Not Fraud', 'Fraud'])\n",
    "g.set_yticklabels([])\n",
    "\n",
    "# function to show values on bars\n",
    "def show_values_on_bars(axs):\n",
    "    def _show_on_single_plot(ax):        \n",
    "        for p in ax.patches:\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            _y = p.get_y() + p.get_height()\n",
    "            value = '{:.0f}'.format(p.get_height())\n",
    "            ax.text(_x, _y, value, ha=\"center\") \n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _show_on_single_plot(ax)\n",
    "    else:\n",
    "        _show_on_single_plot(axs)\n",
    "show_values_on_bars(ax)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.title('Distribution of Transactions', fontsize=30)\n",
    "plt.tick_params(axis='x', which='major', labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17304750013189596"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print percentage of questions where target == 1\n",
    "(len(df.loc[df.Class==1]))/(len(df.loc[df.Class==0]))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see we have a very imbalanced class - just 0.17% of our dataset belong to the target class!\n",
    "\n",
    "This is a problem because many machine learning models are designed to maximize overall accuracy, which especially with imbalanced classes may not be the best metric to use. Classification accuracy is defined as the number of correct predictions divided by total predictions times 100. For example, if we simply predicted all transactions are not fraud, we would get a classification acuracy score of over 99%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# Separate input features and target\n",
    "y=df.Class\n",
    "X=df.drop('Class',axis=1)\n",
    "\n",
    "# setting up train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicted labels:  [0]\n",
      "Test score:  0.9982865649841297\n"
     ]
    }
   ],
   "source": [
    "# baseline model\n",
    "dummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "dummy_pred = dummy.predict(X_test)\n",
    "\n",
    "# checking unique labels\n",
    "print('Unique predicted labels: ', (np.unique(dummy_pred)))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test score: ', accuracy_score(y_test, dummy_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted our accuracy score for classifying all transactions as not fraud is 99.8%!\n",
    "\n",
    "As the Dummy Classifier predicts only Class 0, it is clearly not a good option for our objective of correctly classifying fraudulent transactions.\n",
    "\n",
    "Let's see how logistic regression performs on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9989326142524086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    71116\n",
       "1       86\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modeling the data as is\n",
    "# Train model\n",
    "lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "# predict on training set \n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "#checking accuracy \n",
    "print(accuracy_score(y_test, lr_pred))\n",
    "\n",
    "# checking unique values\n",
    "predictions = pd.DataFrame(lr_pred)\n",
    "#predictions.head()\n",
    "predictions[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe not surprisingly, our accuracy score decreased as compared to the dummy classifier above. This tells us that either we did something wrong in our logistic regression model, or that accuracy might not be our best option for measuring performance.\n",
    "\n",
    "Letâ€™s take a look at some popular methods for dealing with class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Change the performance metric`\n",
    "As we saw above, accuracy is not the best metric to use when evaluating imbalanced datasets as it can be very misleading. Metrics that can provide better insight include:\n",
    "###### Confusion Matrix: \n",
    "a table showing correct predictions and types of incorrect predictions.\n",
    "###### Precision: \n",
    "the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifierâ€™s exactness. Low precision indicates a high number of false positives.\n",
    "###### Recall: \n",
    "the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifierâ€™s completeness. Low recall indicates a high number of false negatives.\n",
    "###### F1: Score: \n",
    "the weighted average of precision and recall.\n",
    "\n",
    "Letâ€™s see what happens when we apply these F1 and recall scores to our logistic regression from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:    0.6346153846153847\n",
      "recall_score: 0.5409836065573771\n",
      "accuracy_score: 0.9989326142524086\n"
     ]
    }
   ],
   "source": [
    "def f1_recall_accuracy_score(y_test,model_pred):\n",
    "    # f1 score\n",
    "    print(f\"f1_score:    \" + str(f1_score(y_test, model_pred)))\n",
    "\n",
    "    # recall score\n",
    "    print(\"recall_score: \" + str(recall_score(y_test, model_pred)))\n",
    "    \n",
    "    # accuracy score\n",
    "    print(\"accuracy_score: \" + str(accuracy_score(y_test, model_pred)))\n",
    "\n",
    "f1_recall_accuracy_score(y_test,lr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `Change the algorithm`\n",
    "While in every machine learning problem, itâ€™s a good rule of thumb to try a variety of algorithms, it can be especially beneficial with imbalanced datasets. Decision trees frequently perform well on imbalanced data. They work by learning a hierarchy of if/else questions and this can force both classes to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71080\n",
      "           1       0.94      0.71      0.81       122\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.97      0.86      0.90     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# train model\n",
    "rfc = RandomForestClassifier(n_estimators=10).fit(X_train,y_train)\n",
    "\n",
    "# predict on test set\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:    0.8093023255813954\n",
      "recall_score: 0.7131147540983607\n",
      "accuracy_score: 0.9994241734782731\n"
     ]
    }
   ],
   "source": [
    "f1_recall_accuracy_score(y_test,rfc_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both F1 and recall have increased as compared to logistic regression! It appears that for this specific problem, random forest may be a better choice of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `Resampling Techniques â€” Oversample minority class`\n",
    "\n",
    "Our next method begins our resampling techniques.\n",
    "Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you donâ€™t have a ton of data to work with.\n",
    "We will use the resampling module from Scikit-Learn to randomly replicate samples from the minority class.\n",
    "\n",
    "##### Important Note\n",
    "\n",
    "Always split into test and train sets BEFORE trying oversampling techniques! Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets. This can allow our model to simply memorize specific data points and cause overfitting and poor generalization to the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate input features and target\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    284315\n",
       "0    284315\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate minority and majority classes\n",
    "not_fraud = df[df.Class==0]\n",
    "fraud = df[df.Class==1]\n",
    "\n",
    "# upsample minority\n",
    "fraud_upsampled = resample(fraud,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(not_fraud), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_fraud, fraud_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "upsampled.Class.value_counts()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After resampling we have an equal ratio of data points for each class! Letâ€™s try our logistic regression again with the balanced training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:    0.11289537712895377\n",
      "recall_score: 0.8787878787878788\n",
      "accuracy_score: 0.9743967866071178\n"
     ]
    }
   ],
   "source": [
    "# trying logisticregression again with the balanced data set \n",
    "y_train = upsampled.Class\n",
    "X_train = upsampled.drop('Class', axis=1)\n",
    "\n",
    "# model\n",
    "upsampled_lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "upsampled_pred = upsampled_lr.predict(X_test)\n",
    "\n",
    "# checking classification scores\n",
    "f1_recall_accuracy_score(y_test,upsampled_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our recall score increased, but F1 is much lower than with either our baseline logistic regression or random forest from above. Letâ€™s see if undersampling might perform better here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `Resampling techniques â€” Undersample majority class`\n",
    "\n",
    "Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set.\n",
    "We will again use the resampling module from Scikit-Learn to randomly remove samples from the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492\n",
       "0    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still using our separated classes fraud and not_fraud from above\n",
    "\n",
    "# downsample majority\n",
    "not_fraud_downsampled = resample(not_fraud,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(fraud), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_fraud_downsampled, fraud])\n",
    "\n",
    "# checking counts\n",
    "downsampled.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have an equal ratio of fraud to not fraud data points, but in this case a much smaller quantity of data to train the model on. Letâ€™s again apply our logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:    0.12413055109684323\n",
      "recall_score: 0.8787878787878788\n",
      "accuracy_score: 0.9770090727788545\n"
     ]
    }
   ],
   "source": [
    "# trying logistic regression again with the undersampled dataset\n",
    "\n",
    "y_train = downsampled.Class\n",
    "X_train = downsampled.drop('Class', axis=1)\n",
    "\n",
    "undersampled_lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "undersampled_pred = undersampled_lr.predict(X_test)\n",
    "\n",
    "# checking classification scores\n",
    "f1_recall_accuracy_score(y_test,undersampled_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling underperformed oversampling in this case. Letâ€™s try one more method for handling imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. `Generate synthetic samples`\n",
    "\n",
    "A technique similar to upsampling is to create synthetic samples. Here we will use imblearnâ€™s SMOTE or Synthetic Minority Oversampling Technique. SMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model.\n",
    "\n",
    "Again, itâ€™s important to generate the new samples only in the training set to ensure our model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate input features and target\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "#sm = SMOTE(random_state=27, ratio=1.0)\n",
    "sm = SMOTE(random_state=27)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating our synthetic data points, letâ€™s see how our logistic regression performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:    0.12413055109684323\n",
      "recall_score: 0.8787878787878788\n",
      "accuracy_score: 0.9770090727788545\n"
     ]
    }
   ],
   "source": [
    "smote_lr = LogisticRegression(solver='liblinear').fit(X_train,y_train)\n",
    "\n",
    "smote_pred = smote_lr.predict(X_test)\n",
    "\n",
    "# checking classification scores\n",
    "f1_recall_accuracy_score(y_test,undersampled_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explored 5 different methods for dealing with imbalanced datasets:\n",
    "- Change the performance metric\n",
    "- Change the algorithm\n",
    "- Oversample minority class\n",
    "- Undersample majority class\n",
    "- Generate synthetic samples using SMOTE \n",
    "\n",
    "It appears for this particular dataset random forest and SMOTE are among the best of the options we tried here.\n",
    "\n",
    "These are just some of the many possible methods to try when dealing with imbalanced datasets, and not an exhaustive list. Some others methods to consider are collecting more data or choosing different resampling ratios â€” you donâ€™t have to have exactly a 1:1 ratio!\n",
    "\n",
    "You should always try several approaches and then decide which is best for your problem.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
